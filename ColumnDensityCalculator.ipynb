{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c130a5d-2048-4440-a873-2e74eb1dc39d",
   "metadata": {},
   "source": [
    "# Calculating column density from a $^{13}CO$ observation of MAXI J1348-630\n",
    "\n",
    "In this document we will go through several steps taken to get a final $^{13}CO$ column density map. There are several assumptions that need to be revisited for future use of this document if more data is available. They will be adressed when they appear.\n",
    "\n",
    "The code was produced by Dr. Alexandra Tetarenko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67c4776c-51e9-4535-9225-0f294b268832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral_cube import SpectralCube\n",
    "from astropy import units as u\n",
    "import astropy.constants as con\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import pylab as pl\n",
    "import math as ma\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import hstack\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset,zoomed_inset_axes\n",
    "import matplotlib.cm as cm\n",
    "from astropy import wcs\n",
    "from astropy.coordinates import SkyCoord,Angle\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import mpmath\n",
    "from astropy.table import hstack\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Rectangle\n",
    "import aplpy\n",
    "from matplotlib import rc\n",
    "import os\n",
    "from astropy.convolution import Gaussian1DKernel\n",
    "import radio_beam\n",
    "\n",
    "\n",
    "datadir = '/Users/student/Desktop/localdata/MAXI_J1348/data/' #We first define the data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a6e84-f7e1-4b91-94db-b561f31e3170",
   "metadata": {},
   "source": [
    "## Selecting the velocity range:\n",
    "\n",
    "Given that your observation might feature foreground and background emission outside of a given velocity range, it is important to first get the interesting data out of the full cube.\n",
    "\n",
    "Also ,we apply a unit conversion when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de037b70-1786-411d-9850-82f5d2aa0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVarr(fitsfile):\n",
    "    header = fits.getdata(fitsfile, header=True)[1]  # We import the fits header.\n",
    "    X = SpectralCube.read(fitsfile)  # We read the file directly with the spectral cube repository.\n",
    "    X.allow_huge_operations = True  # This allows operations that might exceed memory limits.\n",
    "\n",
    "    print(X.unit)  # Check the unit of the FITS file.\n",
    "\n",
    "    if X.unit == u.K:\n",
    "        X1 = X  # No conversion needed.\n",
    "        print('No conversion needed')\n",
    "    else:\n",
    "        X1 = X.to(u.K, equivalencies=X.beam.jtok_equiv(X.header['RESTFRQ'] * u.Hz))\n",
    "        print('Conversion applied')\n",
    "\n",
    "    Xkms = X1.with_spectral_unit(u.km/u.s, velocity_convention='radio')  # We transform the spectral axis to km/s.\n",
    "    Xkms2 = Xkms.spectral_slab(vmin * u.km/u.s, vmax * u.km/u.s)  # We take a given width of velocity values.\n",
    "    Vel = np.array(Xkms2.spectral_axis)  # We extract the subspectral axis with the desired velocities.\n",
    "    Ts = np.array(Xkms2)  # We now extract the temperatures associated with such velocities.\n",
    "\n",
    "    return Ts[::-1, :, :], Vel[::-1]  # We return the values in inverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03833a52-e767-4007-96e1-c08969913932",
   "metadata": {},
   "source": [
    "## Making a $T_{max}$ map:\n",
    "\n",
    "In this step we take our cube slice and create a $T_{max}$ map of the emission. Notice, that to get that value we would normally use our $^{12}CO$ observation of that same transition, which is optically thick, but since we don't have it, we take the approach to use that very same observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6fcf524-5879-4023-b6e5-4398cccc1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TMAX(fitsfile,ddir,line,vmin,vmax):\n",
    "\theader = fits.getdata(fitsfile, header=True)[1] #Again, we read the header\n",
    "\treadfile=SpectralCube.read(fitsfile) #We open the fits file with SpectralCube\n",
    "\treadfile.allow_huge_operations=True #We enable operations beyond memory limits\n",
    "\tfilekms=readfile.with_spectral_unit(u.km/u.s,velocity_convention='radio') #We read the file with the correct velocity units.\n",
    "\tfileKkms=filekms.to(u.K, equivalencies=filekms.beam.jtok_equiv(filekms.header['RESTFRQ']*u.Hz)) #We transform the emission units to K\n",
    "\tfileKkms2=fileKkms.spectral_slab(vmin*u.km/u.s,vmax*u.km/u.s) #We select a chunk of velocity.\n",
    "\ttmax=fileKkms2.apply_numpy_function(np.nanmax,axis=0) #We select the maximum value of the array for the velocity axis, effectively collapsing the cube over the spectral axis.\n",
    "\ttmax_fix=np.nan_to_num(tmax) #We remove the NaNs and transform them into numbers (0s). \n",
    "\tfits.writeto(filename=ddir+'T_max'+line+'.fits',output_verify='ignore',overwrite=True,data=tmax_fix,header=header) #We create the Tmax array into a fits file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17652b99-e397-4b84-bbfc-1a53d2911baf",
   "metadata": {},
   "source": [
    "## Making a $T_{ex}$ map:\n",
    "\n",
    "In this step, we take our $T_{max}$ map, and use it to create a $T_{ex}$ map. We will be assuming an optically thick region with an observational formula approach found on Tetarenko et al., 2017:\n",
    "\n",
    "$T^{obs}_{ex}=\\frac{h\\nu/k}{\\ln \\left(1+\\frac{h\\nu/k}{T_{max}+ h\\nu/k C_{CMB}}\\right)}, $ for $\\tau \\rightarrow \\infty$,\n",
    "\n",
    "where $C_{CMB}=\\left[\\exp \\left(\\frac{h\\nu}{kT_{CMB}}\\right)-1\\right]^{-1}$.\n",
    "\n",
    "Noticeably, our $^{13}CO$ observation is not optically thick, hence the validity of this equation is really not quite accurate. However, in lack of a $^{12}CO$ observation for that same transition, this is all that we have. Notice that $T_R$ is our observed temperature at each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc314105-8f58-4e4c-b7c1-40024690e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tex(nu0,Tmax0):\n",
    "\t\"\"\"\n",
    "    nu0 is reference frequency molecule in Ghz,Tmax0 is reference molecule max T in K\n",
    "    \"\"\"\n",
    "\tunitdict={'h':6.626068e-27,'k':1.3806503e-16,'c':2.99792458e10} #We introduce numerical values for Planck's constant, speed of light and Boltzmann's constant.\n",
    "\th,k,c=unitdict['h'],unitdict['k'],unitdict['c']\n",
    "\tnu=nu0*1e9 #We transform the frequency to Hz\n",
    "\tTbg=2.73 #Temperature of the CMB background to compute its blackbody emission.\n",
    "\tC=h*nu/k #We define the C value that will appear more than once in our formulas to make them simple.\n",
    "\tCbg=1./(np.exp((h*nu)/(k*Tbg))-1) #We compute the term associated to the background emission.\n",
    "\tTex=C/(np.log(1+(C/(Tmax0+C*Cbg)))) #Finally, we derive values for the excitation temp.\n",
    "\treturn(Tex) #We return the array with the computed temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdcd86-0f03-4c7b-9715-41b160557660",
   "metadata": {},
   "source": [
    "## Estimating optical depth:\n",
    "\n",
    "At this point we have to produce a $\\tau$ map. Normally, this is performed when one can assess more than one observation with different species and get $T_ex$ with the optically thick one. But in this case, again, we only have $^{13}CO$, so we will have to make do with what we have.\n",
    "\n",
    "We use the same formula found in Tetarenko et al., 2017:\n",
    "\n",
    "\n",
    "$\\tau = -\\ln \\left( 1 - \\frac{kT_R}{h \\nu} \\left( \\frac{1}{\\exp\\left[\\frac{h \\nu}{k T_{\\text{ex}}}\\right] - 1} - \\frac{1}{\\exp\\left[\\frac{h \\nu}{k T_{\\text{bg}}}\\right] - 1} \\right)^{-1} \\right)$\n",
    "\n",
    "The result of this function is somewhat patchy, with a not so good looking map (a lot of variations), so don't worry if that happens to your data too as long as the peaks of depth are associated to the highest emission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2048b66-ff8e-4994-ae8a-71ba1e990867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tau(nu1,TV,Tex):\n",
    "\t#nu1 is target frequency molecule in ghz,TV is array of target molecule T in K for a bunch of dV\n",
    "\tunitdict={'h':6.626068e-27,'k':1.3806503e-16,'c':2.99792458e10}\n",
    "\th,k,c=unitdict['h'],unitdict['k'],unitdict['c']\n",
    "\tnu=nu1*1e9 #Transform the frequency to Hz\n",
    "\tTbg=2.73 #Define the temperature of the CMB\n",
    "\tC=h*nu/k #Use a simplifying factor for future formulas.\n",
    "\tA=1./(np.exp(C/Tex)-1) #We name the term associated with Tex\n",
    "\tB=1./(np.exp(C/Tbg)-1) #We do the same for that term associated with the CMB temp.\n",
    "\ttau=-np.log(1-((TV/C)*((A-B)**(-1)))) #We finally take the tau full estimation starting from our Tex map.\n",
    "\treturn(tau) #tau is in the shape of a spectral subcube for given velocity channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2e80b-e219-4beb-8e9c-be6ec901b6c8",
   "metadata": {},
   "source": [
    "## Computing the column density of a given species:\n",
    "\n",
    "In this bit we calculate the column density of a given level J, which we will later transform into the total column density of the molecule. We take from Wilson et al., 2013 the following expression:\n",
    "\n",
    "\n",
    "$N_J = \\frac{8 \\pi \\nu^3}{c^3} \\frac{(2J+1)}{(J+1)} (1.165 \\times 10^{11}) \\frac{1}{\\mu^2}  \\left(1 - e^{-\\frac{h \\nu}{k T_{\\text{ex}}}}\\right)^{-1} \\int_{v_{min}}^{v_{max}} \\tau dv$\n",
    "\n",
    "\n",
    "where:\n",
    "- ( $J$) → Rotational quantum number.\n",
    "- ( $\\mu$) → Dipole moment of the molecule (in Debye).\n",
    "- ( $T_{\\text{ex}}$) → Excitation temperature (in K).\n",
    "- The numeric constant is a combination of the Einstein factor for the given transition ($A_{ul}$) and the population degeneracies for the given levels ($g_u, g_l$). **Ask Alex about this, because the number seems odd in 2017 paper**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d2d3104-5647-4e97-b12d-8c613239aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NJ(nu1,Tex,Tau_array,dV,mu,J):\n",
    "\t#TV is array of target molecule T in K for a bunch of dV (array also in km/s)\n",
    "\tunitdict={'h':6.626068e-27,'k':1.3806503e-16,'c':2.99792458e10}\n",
    "\th,k,c=unitdict['h'],unitdict['k'],unitdict['c']\n",
    "\tnu=nu1*1e9\n",
    "\tprefac = (8 * np.pi * 1*u.GHz**3 / (con.c**3)) * u.km/u.s * u.s #we compute the prefactor for the final formula assigning the correct units\n",
    "\tC=(prefac.to(u.cm**-2)).value #we transform to cm since our final values will be in cm**{-2}\n",
    "\tcol=C*((2*J+1)/(J+1))*(0.858e11)*(mu)**(-2)*(1-np.exp(-h*nu/(k*Tex)))**(-1) #We compute the scaling factor used on tau to get Nj\n",
    "\tinte=np.trapz(np.nan_to_num(Tau_array),dV,axis=0) #We integrade optical depth over the velocity axis using the trapezoidal method.\n",
    "\tfits.writeto(filename=datadir+'column_plots/'+line+'_inte.fits',output_verify='ignore', overwrite=True,data=inte,header=header) #We write a fits file with the N_j\n",
    "\treturn(col*inte) #We return the scaled factor, so an N_j matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0c61e-84aa-4604-8374-1c5e4111c102",
   "metadata": {},
   "source": [
    "## Computing the partition function for a give $T_{ex}$ map:\n",
    "\n",
    "In order to extrapolate the column density of an energy level J to a full molecule we must take into account the partition function for all the different rotational levels. In this case we take for each $T_{ex}$ value in our map:\n",
    "\n",
    "$$Z (T_{ex}) = \\sum_{J=0}^{6}(2J+1)\\cdot e^{-\\frac{2.65 J(J+1)}{T_{ex}}}$$\n",
    "\n",
    "Where we take into account the first 6 rotational levels of the $^{13}CO$ molecule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f920a8bc-9ca6-4c53-8cd9-0ddb8fd58753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zpart(Tex_array):\n",
    "\tZZ=np.zeros((Tex_array.shape[0],Tex_array.shape[1]))\n",
    "\tfor kk in range(0,(Tex_array.shape[0])):\n",
    "\t\tfor ll in range(0,(Tex_array.shape[1])):\n",
    "\t\t\tval=Tex_array[kk,ll]\n",
    "\t\t\tZZ[kk,ll]=mpmath.nsum(lambda x: (2*x+1)*mpmath.exp(-2.65*x*(x+1)/val), [0,6])\n",
    "\treturn(ZZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f395ea-f96e-4758-a724-a4d87bd486e6",
   "metadata": {},
   "source": [
    "## Transforming $N_{J}$ to $N_{tot}$:\n",
    "\n",
    "In this last part we take our $N_J$ map and transform it into $N_{tot}$ by taking into account our full partition function:\n",
    "\n",
    "$$N_{tot} = N_J \\cdot\\frac{Z(T_{ex})}{2J+1}\\cdot e^{\\frac{2.65 J(J+1)}{T_{ex}}}$$\n",
    "\n",
    "Where the 2.65 value is the $hB_e/k = 2.65 K$ ratio specific to $CO$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d42f6365-9594-4375-83a7-eb062dab7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ntot(Tex,Nj,J):\n",
    "\tZZ=Zpart(Tex)/(2*J+1)\n",
    "\tNN=Nj*ZZ*np.exp(2.65*(J*(J+1.))/Tex)\n",
    "\treturn(NN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41819721-d996-42da-9fc8-29859888ecec",
   "metadata": {},
   "source": [
    "## Final production of column density maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1eb0721-5de6-4b4e-b23c-ef987ae24f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "TMAX map done.\n",
      "TMAX fits done.\n",
      "Info input done.\n",
      "Tex map done.\n",
      "K\n",
      "No conversion needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: PossiblySlowWarning: This function (<function BaseSpectralCube.to at 0x1756b7a60>) requires loading the entire cube into memory and may therefore be slow. [spectral_cube.utils]\n",
      "/Users/student/anaconda3/lib/python3.11/site-packages/spectral_cube/spectral_cube.py:439: RuntimeWarning: All-NaN slice encountered\n",
      "  out = function(self._get_filled_data(fill=fill,\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_1 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_2 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card TIMESYS [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_1 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_2 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card TIMESYS [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_1 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_2 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card TIMESYS [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_1 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card PV2_2 [spectral_cube.wcs_utils]\n",
      "WARNING: WCSWarning: WCS1 is missing card TIMESYS [spectral_cube.wcs_utils]\n",
      "/var/folders/2l/vhjdf14d2vjd4qffvsk5p19r0000gq/T/ipykernel_12879/191859481.py:10: RuntimeWarning: invalid value encountered in log\n",
      "  tau=-np.log(1-((TV/C)*((A-B)**(-1)))) #We finally take the tau full estimation starting from our Tex map.\n",
      "/var/folders/2l/vhjdf14d2vjd4qffvsk5p19r0000gq/T/ipykernel_12879/191859481.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  tau=-np.log(1-((TV/C)*((A-B)**(-1)))) #We finally take the tau full estimation starting from our Tex map.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tex fits done.\n",
      "Tau map done.\n",
      "Tau fits done.\n",
      "NJ fits done.\n",
      "NTOT fits done.\n"
     ]
    }
   ],
   "source": [
    "Tex_vec=np.vectorize(Tex)\n",
    "Tau_vec=np.vectorize(Tau)\n",
    "\n",
    "\n",
    "\n",
    "#per spectra version\n",
    "\n",
    "fitsfile='/Users/student/Desktop/localdata/MAXI_J1348/data/J1348_630_12m+7m_13co10.fits' #We point to the file\n",
    "\n",
    "\n",
    "line='CO13_comb'\n",
    "\n",
    "dipolem=0.122#CO\n",
    "moment=0\n",
    "flag='K'\n",
    "vmin=-54 #Minimum velocity of our original observation cube\n",
    "vmax=-50 #Maximum velocity of our original observation cube\n",
    "cell=0.5#CO--1.5/0.5\n",
    "\n",
    "print('Starting...')\n",
    "\n",
    "make_TMAX(fitsfile,datadir+'column_plots/',line,vmin,vmax)\n",
    "print('TMAX map done.')\n",
    "tmax_fits=datadir+'column_plots/'+'T_max'+line+'.fits'\n",
    "print('TMAX fits done.') \n",
    "#We make and save the Tmax map\n",
    "\n",
    "hdtmax=fits.open(tmax_fits)[0]\n",
    "dat=np.nan_to_num(hdtmax.data)\n",
    "#We open our Tmax and correct the NaNs to 0s\n",
    "\n",
    "freq=110.20135#CO13 1-0 transition frequency\n",
    "J=0#J+1->J\n",
    "fr=np.empty((dat.shape[0],dat.shape[1]))\n",
    "fr.fill(freq) #We create an array with the shape of the T_max map filled with the transition frequency value\n",
    "tm=np.zeros((dat.shape[0],dat.shape[1]))\n",
    "fr=np.zeros((dat.shape[0],dat.shape[1]))\n",
    "#We create empty 0 matrices that we will use in a minute\n",
    "\n",
    "print('Info input done.')\n",
    "\n",
    "for kk in range(0,(dat.shape[0])):\n",
    "\tfor ll in range(0,(dat.shape[1])):\n",
    "\t\tval=dat[kk,ll]\n",
    "\t\ttm[kk,ll]=val\n",
    "\t\tfr[kk,ll]=freq\n",
    "        #We use our empty matrices to fill them with frequency and temperature values.\n",
    "        \n",
    "Tex_array=Tex_vec(fr,tm)\n",
    "\n",
    "#We start executing our functions to get to our final Ntot calculation.\n",
    "print('Tex map done.')\n",
    "header = fits.getdata(tmax_fits, header=True)[1]\n",
    "fits.writeto(filename=datadir+'column_plots/'+line+'_tex.fits',output_verify='ignore',\\\n",
    "\toverwrite=True,data=Tex_array,header=header)\n",
    "TV,dV=makeVarr(fitsfile)\n",
    "print('Tex fits done.')\n",
    "Tau_array=Tau_vec(fr,TV,Tex_array)\n",
    "print('Tau map done.')\n",
    "Tau_array[Tau_array == np.inf] = np.nanmax(Tau_array[Tau_array != np.inf])\n",
    "fits.writeto(filename=datadir+'column_plots/'+line+'_tau.fits',output_verify='ignore',\\\n",
    "\toverwrite=True,data=Tau_array,header=header)\n",
    "print('Tau fits done.')\n",
    "NJ_array=NJ(fr,Tex_array,Tau_array,dV,dipolem,J)\n",
    "fits.writeto(filename=datadir+'column_plots/'+line+'_nj.fits',output_verify='ignore',\\\n",
    "\toverwrite=True,data=NJ_array,header=header)\n",
    "Nfact_array=Ntot(Tex_array,NJ_array,J)\n",
    "print('NJ fits done.')\n",
    "\n",
    "colum=Nfact_array\n",
    "#0.2 for Co\n",
    "colum[tm<0.2]=0. #We filter our matrix with a hard threshold of 0.2\n",
    "\n",
    "fits.writeto(filename=datadir+'column_plots/'+line+'_column.fits',output_verify='ignore',\\\n",
    "\toverwrite=True,data=colum,header=header)\n",
    "\n",
    "print('NTOT fits done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca5a9a-6c27-49c4-8494-437355e4c7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
